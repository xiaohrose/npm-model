import { OpenAI } from "openai";
import fs from 'fs';
import path from 'path';
import { getModelConfig } from './constants';
import { omit } from 'lodash'
import { IModelConfig } from '@/types'
import os from 'os';

const configDir = path.join(os.homedir(), '.mc-mp');

// è·å–å‘½ä»¤è¡Œå‚æ•°
const params = process.argv.slice(2);
const modelIndex = params.indexOf('-t');
let model, MAX_HISTORY = 0;

if (params.length === 0) {
    console.error('no content');
    process.exit(1);
}

if (!process.env.KEY) {
    console.error('no KEY');
    process.exit(1);
}

if (modelIndex >= 0) {
    model = params[modelIndex + 1];
    params.splice(modelIndex, 2);
}
const historyCountIndex = params.indexOf('-n');

if (historyCountIndex >= 0) {
    MAX_HISTORY = parseInt(params[historyCountIndex + 1]);
    params.splice(historyCountIndex, 2);
}

const currentModel = getModelConfig(model);

if (!currentModel.key || !currentModel.url) {
    console.error('no model');
    process.exit(1)
}

// åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
const openai = new OpenAI({
    baseURL: currentModel.url,
    apiKey: process.env[currentModel.key]
});

// ç¡®ä¿ç›®å½•å­˜åœ¨
if (!fs.existsSync(path.join(configDir, 'chats'))) {
    fs.mkdirSync(path.join(configDir, 'chats'), { recursive: true });
}

// å°† HISTORY_FILE çš„è·¯å¾„ä¿®æ”¹ä¸ºç›¸å¯¹äºå½“å‰é¡¹ç›®ç›®å½•ï¼Œå¹¶åœ¨æ–‡ä»¶åä¸­åŠ å…¥æ¨¡å‹ä¿¡æ¯
const HISTORY_FILE = path.join(configDir, 'chats', `${new Date().toISOString().split('T')[0]}_${currentModel.model}_chat_history.json`);

// æ·»åŠ æ¥å£å®šä¹‰
interface HistoryEntry {
    role: 'system' | 'user' | 'assistant';
    content: string;
    name?: string;
    timestamp?: number;
    reasoning_content?: string;
}

// è¯»å–å†å²è®°å½•
export function loadHistory(): HistoryEntry[] {
    try {
        if (!fs.existsSync(HISTORY_FILE)) {
            // å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºæ•°ç»„å¹¶ä¿å­˜
            saveHistory([]);
            return [];
        }

        const data = fs.readFileSync(HISTORY_FILE, 'utf8');
        let history: HistoryEntry[] = [];
        try {
            history = JSON.parse(data) as HistoryEntry[];
        } catch (error) {
            console.error('Error parsing history:', error);
        }
        return history;
    } catch (error) {
        console.error('Error loading history:', error);
    }
    return [];
}

// ä¿å­˜å†å²è®°å½•
export function saveHistory(history: OpenAI.Chat.ChatCompletionMessageParam[]) {
    try {
        fs.writeFileSync(HISTORY_FILE, JSON.stringify(history, null, 2));
    } catch (error) {
        console.error('Error saving history:', error);
    }
}

// å®šä¹‰æ¶ˆæ¯å¤„ç†å‡½æ•°
export async function handleStreamResponse(completion: any, callback: (data: { content: string; reasoning_content: string }) => void, model: IModelConfig) {
    let isHeader = true;
    try {
        for await (const chunk of completion.iterator()) {
            // æ£€æŸ¥æ˜¯å¦ä¸­æ­¢
            if (completion.controller.signal.aborted) {
                console.log('Stream processing aborted!');
                return;
            }

            const content = chunk.choices[0]?.delta?.content;
            const reasoning_content = chunk.choices[0]?.delta?.reasoning_content;
            // å¤„ç†å¤´éƒ¨æ¢è¡Œ
            if (isHeader) {
                if (content !== '\n') {
                    isHeader = false;
                    process.stdout.write('\n');
                }
                continue;
            }

            // å¤„ç†å†…å®¹è¾“å‡º
            if (content || reasoning_content) {
                process.stdout.write(content || reasoning_content);
                callback({ content, reasoning_content });
            }

            // å¤„ç†ç»“æŸæ ‡è®°
            if (chunk.choices[0]?.finish_reason === 'stop') {
                process.stdout.write('\n' + model.model + ' ğŸ˜›\n');
                break; // æ˜ç¡®ç»“æŸå¾ªç¯
            }
        }
    } catch (error) {
        console.error('Error processing stream:', error);
    }
}

export async function main() {
    try {
        // åŠ è½½å†å²è®°å½•
        let messageHistory = loadHistory();

        // ä¿æŒå†å²è®°å½•åœ¨æœ€å¤§é•¿åº¦é™åˆ¶å†…
        let latestMessageHistory = MAX_HISTORY ? messageHistory.slice(-(MAX_HISTORY * 2)) : [];

        const userMessage: HistoryEntry = {
            role: 'user' as const,
            content: params.join(' '),
            timestamp: Date.now()
        }
        // æ·»åŠ ç”¨æˆ·æ–°æ¶ˆæ¯åˆ°å†å²è®°å½•ï¼ˆæ·»åŠ æ—¶é—´æˆ³ï¼‰
        latestMessageHistory.push(userMessage);

        latestMessageHistory = latestMessageHistory.map(item => omit(item, ['reasoning_content', 'timestamp']));

        const completion = await openai.chat.completions.create({
            messages: [
                { role: "system", content: "ä½ æ˜¯ä¸€ä¸ªæ‹¥æœ‰ä¸“ä¸šçŸ¥è¯†çš„åŠ©æ‰‹ï¼Œæ¯æ¬¡åœ¨ç”¨æˆ·æå‡ºé—®é¢˜æ—¶ï¼Œéœ€è¦è´¨ç–‘ç”¨æˆ·çš„é—®é¢˜ï¼Œå¹¶æŒ‡æ­£å‡ºå½“å‰é—®é¢˜æ€æ ·é—®æ›´åŠ ç³»ç»Ÿ,å¹¶ç»™å‡ºè¿™æ ·æé—®é¢˜ï¼Œæ›´åŠ ç³»ç»Ÿçš„åŸå› ï¼ˆæœ€å¥½ä¸€ä¸€åˆ—å‡ºæ¥ï¼‰ã€‚ç„¶ååœ¨ç»™å‡ºç”¨æˆ·çš„é—®é¢˜ç­”æ¡ˆï¼Œä¸­é—´ç”¨ä¸€è¡Œ *** å·åˆ†å‰²ã€‚" },
                ...latestMessageHistory
            ],
            model: currentModel.model,
            stream: true
        });

        // ä¿å­˜åŠ©æ‰‹çš„å›å¤åˆ°å†å²è®°å½•ï¼ˆæ·»åŠ æ—¶é—´æˆ³ï¼‰
        const assistantMessage: HistoryEntry = {
            role: 'assistant' as const,
            content: '',
            timestamp: Date.now()
        };
        messageHistory.push(userMessage, assistantMessage);

        // ä¿®æ”¹handleStreamResponseæ¥ç´¯ç§¯å“åº”å†…å®¹
        await handleStreamResponse(completion, (data: { content?: string; reasoning_content?: string }) => {
            data.content && (assistantMessage.content += data.content);
            data.reasoning_content && (assistantMessage.reasoning_content += data.reasoning_content)
        }, currentModel);

        // ä¿å­˜æ›´æ–°åçš„å†å²è®°å½•
        saveHistory(messageHistory);
    } catch (error) {
        console.error('Error in main:', error);
    }
}

main();